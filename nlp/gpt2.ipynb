{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taosy/bin/miniconda3/envs/pt-39-6/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "from transformers import (\n",
    "    # pipeline,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    # AutoModel,\n",
    "    LlamaForCausalLM,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    ")\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", do_lower_case=True, cache_dir=None, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type (var_name):depth-idx)                       Param #\n",
       "================================================================================\n",
       "GPT2LMHeadModel (GPT2LMHeadModel)                       --\n",
       "├─GPT2Model (transformer): 1-1                          --\n",
       "│    └─Embedding (wte): 2-1                             38,597,376\n",
       "│    └─Embedding (wpe): 2-2                             786,432\n",
       "│    └─Dropout (drop): 2-3                              --\n",
       "│    └─ModuleList (h): 2-4                              --\n",
       "│    │    └─GPT2Block (0): 3-1                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-1                  1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-2              --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-1              1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-2              590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-3       --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-4      --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-3                  1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-4                     --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-5                2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-6              2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-7      --\n",
       "│    │    │    │    └─Dropout (dropout): 5-8            --\n",
       "│    │    └─GPT2Block (1): 3-2                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-5                  1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-6              --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-9              1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-10             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-11      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-12     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-7                  1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-8                     --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-13               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-14             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-15     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-16           --\n",
       "│    │    └─GPT2Block (2): 3-3                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-9                  1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-10             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-17             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-18             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-19      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-20     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-11                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-12                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-21               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-22             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-23     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-24           --\n",
       "│    │    └─GPT2Block (3): 3-4                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-13                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-14             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-25             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-26             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-27      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-28     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-15                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-16                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-29               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-30             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-31     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-32           --\n",
       "│    │    └─GPT2Block (4): 3-5                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-17                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-18             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-33             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-34             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-35      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-36     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-19                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-20                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-37               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-38             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-39     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-40           --\n",
       "│    │    └─GPT2Block (5): 3-6                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-21                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-22             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-41             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-42             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-43      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-44     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-23                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-24                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-45               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-46             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-47     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-48           --\n",
       "│    │    └─GPT2Block (6): 3-7                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-25                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-26             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-49             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-50             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-51      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-52     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-27                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-28                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-53               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-54             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-55     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-56           --\n",
       "│    │    └─GPT2Block (7): 3-8                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-29                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-30             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-57             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-58             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-59      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-60     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-31                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-32                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-61               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-62             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-63     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-64           --\n",
       "│    │    └─GPT2Block (8): 3-9                          --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-33                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-34             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-65             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-66             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-67      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-68     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-35                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-36                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-69               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-70             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-71     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-72           --\n",
       "│    │    └─GPT2Block (9): 3-10                         --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-37                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-38             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-73             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-74             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-75      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-76     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-39                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-40                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-77               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-78             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-79     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-80           --\n",
       "│    │    └─GPT2Block (10): 3-11                        --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-41                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-42             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-81             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-82             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-83      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-84     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-43                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-44                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-85               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-86             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-87     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-88           --\n",
       "│    │    └─GPT2Block (11): 3-12                        --\n",
       "│    │    │    └─LayerNorm (ln_1): 4-45                 1,536\n",
       "│    │    │    └─GPT2Attention (attn): 4-46             --\n",
       "│    │    │    │    └─Conv1D (c_attn): 5-89             1,771,776\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-90             590,592\n",
       "│    │    │    │    └─Dropout (attn_dropout): 5-91      --\n",
       "│    │    │    │    └─Dropout (resid_dropout): 5-92     --\n",
       "│    │    │    └─LayerNorm (ln_2): 4-47                 1,536\n",
       "│    │    │    └─GPT2MLP (mlp): 4-48                    --\n",
       "│    │    │    │    └─Conv1D (c_fc): 5-93               2,362,368\n",
       "│    │    │    │    └─Conv1D (c_proj): 5-94             2,360,064\n",
       "│    │    │    │    └─NewGELUActivation (act): 5-95     --\n",
       "│    │    │    │    └─Dropout (dropout): 5-96           --\n",
       "│    └─LayerNorm (ln_f): 2-5                            1,536\n",
       "├─Linear (lm_head): 1-2                                 38,597,376\n",
       "================================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summary(model,depth=8,row_settings=[\"var_names\",\"depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-39-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
