{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "import sys\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "amp_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=None, use_fast=False)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, torch_dtype=amp_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name):depth-idx)                                           Param #\n",
       "====================================================================================================\n",
       "BertForQuestionAnswering (BertForQuestionAnswering)                         --\n",
       "├─BertModel (bert): 1-1                                                     --\n",
       "│    └─BertEmbeddings (embeddings): 2-1                                     --\n",
       "│    │    └─Embedding (word_embeddings): 3-1                                31,254,528\n",
       "│    │    └─Embedding (position_embeddings): 3-2                            524,288\n",
       "│    │    └─Embedding (token_type_embeddings): 3-3                          2,048\n",
       "│    │    └─LayerNorm (LayerNorm): 3-4                                      2,048\n",
       "│    │    └─Dropout (dropout): 3-5                                          --\n",
       "│    └─BertEncoder (encoder): 2-2                                           --\n",
       "│    │    └─ModuleList (layer): 3-6                                         --\n",
       "│    │    │    └─BertLayer (0): 4-1                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-1                        --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-1                    3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-2                     1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-2                  --\n",
       "│    │    │    │    │    └─Linear (dense): 6-3                              4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-4        --\n",
       "│    │    │    │    └─BertOutput (output): 5-3                              --\n",
       "│    │    │    │    │    └─Linear (dense): 6-5                              4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-6                       2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-7                           --\n",
       "│    │    │    └─BertLayer (1): 4-2                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-4                        --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-8                    3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-9                     1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-5                  --\n",
       "│    │    │    │    │    └─Linear (dense): 6-10                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-11       --\n",
       "│    │    │    │    └─BertOutput (output): 5-6                              --\n",
       "│    │    │    │    │    └─Linear (dense): 6-12                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-13                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-14                          --\n",
       "│    │    │    └─BertLayer (2): 4-3                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-7                        --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-15                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-16                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-8                  --\n",
       "│    │    │    │    │    └─Linear (dense): 6-17                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-18       --\n",
       "│    │    │    │    └─BertOutput (output): 5-9                              --\n",
       "│    │    │    │    │    └─Linear (dense): 6-19                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-20                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-21                          --\n",
       "│    │    │    └─BertLayer (3): 4-4                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-10                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-22                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-23                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-11                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-24                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-25       --\n",
       "│    │    │    │    └─BertOutput (output): 5-12                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-26                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-27                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-28                          --\n",
       "│    │    │    └─BertLayer (4): 4-5                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-13                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-29                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-30                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-14                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-31                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-32       --\n",
       "│    │    │    │    └─BertOutput (output): 5-15                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-33                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-34                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-35                          --\n",
       "│    │    │    └─BertLayer (5): 4-6                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-16                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-36                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-37                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-17                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-38                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-39       --\n",
       "│    │    │    │    └─BertOutput (output): 5-18                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-40                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-41                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-42                          --\n",
       "│    │    │    └─BertLayer (6): 4-7                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-19                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-43                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-44                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-20                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-45                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-46       --\n",
       "│    │    │    │    └─BertOutput (output): 5-21                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-47                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-48                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-49                          --\n",
       "│    │    │    └─BertLayer (7): 4-8                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-22                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-50                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-51                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-23                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-52                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-53       --\n",
       "│    │    │    │    └─BertOutput (output): 5-24                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-54                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-55                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-56                          --\n",
       "│    │    │    └─BertLayer (8): 4-9                                         --\n",
       "│    │    │    │    └─BertAttention (attention): 5-25                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-57                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-58                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-26                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-59                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-60       --\n",
       "│    │    │    │    └─BertOutput (output): 5-27                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-61                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-62                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-63                          --\n",
       "│    │    │    └─BertLayer (9): 4-10                                        --\n",
       "│    │    │    │    └─BertAttention (attention): 5-28                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-64                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-65                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-29                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-66                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-67       --\n",
       "│    │    │    │    └─BertOutput (output): 5-30                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-68                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-69                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-70                          --\n",
       "│    │    │    └─BertLayer (10): 4-11                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-31                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-71                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-72                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-32                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-73                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-74       --\n",
       "│    │    │    │    └─BertOutput (output): 5-33                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-75                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-76                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-77                          --\n",
       "│    │    │    └─BertLayer (11): 4-12                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-34                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-78                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-79                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-35                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-80                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-81       --\n",
       "│    │    │    │    └─BertOutput (output): 5-36                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-82                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-83                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-84                          --\n",
       "│    │    │    └─BertLayer (12): 4-13                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-37                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-85                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-86                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-38                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-87                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-88       --\n",
       "│    │    │    │    └─BertOutput (output): 5-39                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-89                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-90                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-91                          --\n",
       "│    │    │    └─BertLayer (13): 4-14                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-40                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-92                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-93                    1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-41                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-94                             4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-95       --\n",
       "│    │    │    │    └─BertOutput (output): 5-42                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-96                             4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-97                      2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-98                          --\n",
       "│    │    │    └─BertLayer (14): 4-15                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-43                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-99                   3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-100                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-44                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-101                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-102      --\n",
       "│    │    │    │    └─BertOutput (output): 5-45                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-103                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-104                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-105                         --\n",
       "│    │    │    └─BertLayer (15): 4-16                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-46                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-106                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-107                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-47                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-108                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-109      --\n",
       "│    │    │    │    └─BertOutput (output): 5-48                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-110                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-111                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-112                         --\n",
       "│    │    │    └─BertLayer (16): 4-17                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-49                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-113                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-114                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-50                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-115                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-116      --\n",
       "│    │    │    │    └─BertOutput (output): 5-51                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-117                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-118                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-119                         --\n",
       "│    │    │    └─BertLayer (17): 4-18                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-52                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-120                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-121                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-53                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-122                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-123      --\n",
       "│    │    │    │    └─BertOutput (output): 5-54                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-124                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-125                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-126                         --\n",
       "│    │    │    └─BertLayer (18): 4-19                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-55                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-127                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-128                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-56                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-129                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-130      --\n",
       "│    │    │    │    └─BertOutput (output): 5-57                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-131                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-132                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-133                         --\n",
       "│    │    │    └─BertLayer (19): 4-20                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-58                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-134                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-135                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-59                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-136                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-137      --\n",
       "│    │    │    │    └─BertOutput (output): 5-60                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-138                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-139                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-140                         --\n",
       "│    │    │    └─BertLayer (20): 4-21                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-61                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-141                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-142                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-62                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-143                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-144      --\n",
       "│    │    │    │    └─BertOutput (output): 5-63                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-145                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-146                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-147                         --\n",
       "│    │    │    └─BertLayer (21): 4-22                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-64                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-148                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-149                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-65                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-150                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-151      --\n",
       "│    │    │    │    └─BertOutput (output): 5-66                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-152                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-153                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-154                         --\n",
       "│    │    │    └─BertLayer (22): 4-23                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-67                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-155                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-156                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-68                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-157                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-158      --\n",
       "│    │    │    │    └─BertOutput (output): 5-69                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-159                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-160                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-161                         --\n",
       "│    │    │    └─BertLayer (23): 4-24                                       --\n",
       "│    │    │    │    └─BertAttention (attention): 5-70                       --\n",
       "│    │    │    │    │    └─BertSelfAttention (self): 6-162                  3,148,800\n",
       "│    │    │    │    │    └─BertSelfOutput (output): 6-163                   1,051,648\n",
       "│    │    │    │    └─BertIntermediate (intermediate): 5-71                 --\n",
       "│    │    │    │    │    └─Linear (dense): 6-164                            4,198,400\n",
       "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-165      --\n",
       "│    │    │    │    └─BertOutput (output): 5-72                             --\n",
       "│    │    │    │    │    └─Linear (dense): 6-166                            4,195,328\n",
       "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-167                     2,048\n",
       "│    │    │    │    │    └─Dropout (dropout): 6-168                         --\n",
       "├─Linear (qa_outputs): 1-2                                                  2,050\n",
       "====================================================================================================\n",
       "Total params: 334,094,338\n",
       "Trainable params: 334,094,338\n",
       "Non-trainable params: 0\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,depth=6,row_settings=[\"var_names\",\"depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(30522, 1024, padding_idx=0)\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n"
     ]
    }
   ],
   "source": [
    "print(model.bert.embeddings.word_embeddings)\n",
    "print(type(model.bert.embeddings.word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================\n",
      "Layer (type (var_name))                  Kernel Shape     Output Shape     Param #          Mult-Adds\n",
      "========================================================================================================\n",
      "LSTMNet (LSTMNet)                        --               [100, 20]        --               --\n",
      "├─Embedding (embedding)                  --               [1, 100, 300]    6,000            6,000\n",
      "│    └─weight                            [300, 20]                         └─6,000\n",
      "├─LSTM (encoder)                         --               [1, 100, 512]    3,768,320        376,832,000\n",
      "│    └─weight_ih_l0                      [2048, 300]                       ├─614,400\n",
      "│    └─weight_hh_l0                      [2048, 512]                       ├─1,048,576\n",
      "│    └─bias_ih_l0                        [2048]                            ├─2,048\n",
      "│    └─bias_hh_l0                        [2048]                            ├─2,048\n",
      "│    └─weight_ih_l1                      [2048, 512]                       ├─1,048,576\n",
      "│    └─weight_hh_l1                      [2048, 512]                       ├─1,048,576\n",
      "│    └─bias_ih_l1                        [2048]                            ├─2,048\n",
      "│    └─bias_hh_l1                        [2048]                            └─2,048\n",
      "├─Linear (decoder)                       --               [1, 100, 20]     10,260           10,260\n",
      "│    └─weight                            [512, 20]                         ├─10,240\n",
      "│    └─bias                              [20]                              └─20\n",
      "========================================================================================================\n",
      "Total params: 3,784,580\n",
      "Trainable params: 3,784,580\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 376.85\n",
      "========================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.67\n",
      "Params size (MB): 15.14\n",
      "Estimated Total Size (MB): 15.80\n",
      "========================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================\n",
       "Layer (type (var_name))                  Kernel Shape     Output Shape     Param #          Mult-Adds\n",
       "========================================================================================================\n",
       "LSTMNet (LSTMNet)                        --               [100, 20]        --               --\n",
       "├─Embedding (embedding)                  --               [1, 100, 300]    6,000            6,000\n",
       "│    └─weight                            [300, 20]                         └─6,000\n",
       "├─LSTM (encoder)                         --               [1, 100, 512]    3,768,320        376,832,000\n",
       "│    └─weight_ih_l0                      [2048, 300]                       ├─614,400\n",
       "│    └─weight_hh_l0                      [2048, 512]                       ├─1,048,576\n",
       "│    └─bias_ih_l0                        [2048]                            ├─2,048\n",
       "│    └─bias_hh_l0                        [2048]                            ├─2,048\n",
       "│    └─weight_ih_l1                      [2048, 512]                       ├─1,048,576\n",
       "│    └─weight_hh_l1                      [2048, 512]                       ├─1,048,576\n",
       "│    └─bias_ih_l1                        [2048]                            ├─2,048\n",
       "│    └─bias_hh_l1                        [2048]                            └─2,048\n",
       "├─Linear (decoder)                       --               [1, 100, 20]     10,260           10,260\n",
       "│    └─weight                            [512, 20]                         ├─10,240\n",
       "│    └─bias                              [20]                              └─20\n",
       "========================================================================================================\n",
       "Total params: 3,784,580\n",
       "Trainable params: 3,784,580\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 376.85\n",
       "========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.67\n",
       "Params size (MB): 15.14\n",
       "Estimated Total Size (MB): 15.80\n",
       "========================================================================================================"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=300, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        out, hidden = self.encoder(embed)\n",
    "        out = self.decoder(out)\n",
    "        out = out.view(-1, out.size(2))\n",
    "        return out, hidden\n",
    "\n",
    "summary(\n",
    "    LSTMNet(),\n",
    "    (1, 100),\n",
    "    dtypes=[torch.long],\n",
    "    verbose=2,\n",
    "    col_width=16,\n",
    "    col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt\", do_lower_case=True, cache_dir=None, use_fast=False)\n",
    "model_gpt = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, torch_dtype=amp_dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-39-6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
